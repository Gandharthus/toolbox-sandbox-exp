from __future__ import annotations

from datetime import date, datetime
from typing import Any, Literal, TypeAlias

from pydantic import BaseModel, Field, ConfigDict, ValidationError, model_validator


# -----------------------------------------------------------------------------
# Shared scalar type
# -----------------------------------------------------------------------------

JsonScalar = str | int | float | bool | date | datetime


# -----------------------------------------------------------------------------
# Leaf query bodies
# -----------------------------------------------------------------------------

class MatchFieldOptions(BaseModel):
    """
    Options for a single field in a `match` query.

    LLM guidance:
      - Use when you need per-field controls like `operator` or `fuzziness`.
      - Example shape: {"match": {"<field>": {"query": "...", "operator": "and"}}}
    """
    query: JsonScalar = Field(..., description="Analyzed input to match against this field.")
    operator: Literal["and", "or"] | None = Field(
        None, description="Logical operator between analyzed terms."
    )
    minimum_should_match: int | str | None = Field(
        None, description='Minimum number/percent of terms required, e.g. 2 or "75%".'
    )
    fuzziness: int | Literal["AUTO"] | None = Field(
        None, description='Allowed edit distance; e.g. "AUTO", 1, 2.'
    )
    prefix_length: int | None = Field(
        None, description="Number of initial characters exempt from fuzziness."
    )
    max_expansions: int | None = Field(
        None, description="Upper bound on term variations generated by fuzziness."
    )
    analyzer: str | None = Field(None, description="Override analyzer for this query.")
    boost: float | None = Field(None, description="Score multiplier for this clause.")

    model_config = ConfigDict(extra="forbid")


class TermValue(BaseModel):
    """
    Verbose object form for a `term` query value (when you need a boost).

    LLM guidance:
      - Compact: {"term": {"status": "active"}}
      - Verbose: {"term": {"status": {"value": "active", "boost": 2.0}}}
    """
    value: JsonScalar = Field(..., description="Exact (non-analyzed) value to match.")
    boost: float | None = Field(None, description="Score multiplier for this term.")
    model_config = ConfigDict(extra="forbid")


class RangeOps(BaseModel):
    """
    Range operators for a single field in a `range` query.

    LLM guidance:
      - Use ISO datetimes or date math (e.g., 'now-7d/d') for time fields.
      - Example: {"range": {"@timestamp": {"gte": "now-7d/d", "lt": "now/d"}}}
    """
    gte: JsonScalar | None = Field(None, description="Greater than or equal.")
    gt: JsonScalar | None = Field(None, description="Greater than (strict).")
    lte: JsonScalar | None = Field(None, description="Less than or equal.")
    lt: JsonScalar | None = Field(None, description="Less than (strict).")
    format: str | None = Field(None, description="Date format string for parsing inputs.")
    time_zone: str | None = Field(None, description="Time zone used when parsing dates.")
    boost: float | None = Field(None, description="Score multiplier for this range.")
    model_config = ConfigDict(extra="forbid")


# -----------------------------------------------------------------------------
# Query containers (each is a one-key object matching ES DSL)
# -----------------------------------------------------------------------------

class MatchQuery(BaseModel):
    """
    Container for a `match` query (single-field full-text).

    JSON shape:
      {"match": {"<field>": <text or MatchFieldOptions>}}

    LLM usage:
      - Default choice for full-text search on one field.
      - For multi-field text, prefer `MultiMatchQuery`.
    """
    match: dict[str, JsonScalar | MatchFieldOptions] = Field(
        ..., description="Map of one field name to its match value/options."
    )
    model_config = ConfigDict(extra="forbid")


class MultiMatchQuery(BaseModel):
    """
    Container for a `multi_match` query (full-text across multiple fields).

    JSON shape:
      {"multi_match": {"query": "...", "fields": ["title^2","body"], "type": "best_fields"}}

    LLM usage:
      - Use for multi-field text; boost via '^' (e.g., 'title^2').
      - `type`: best_fields | most_fields | cross_fields | phrase | phrase_prefix.
    """
    multi_match: dict[str, Any] = Field(
        ..., description="Options for the multi_match query (query, fields, type, etc.)."
    )
    model_config = ConfigDict(extra="forbid")

    @classmethod
    def build(
        cls,
        query: str,
        fields: list[str],
        type: Literal["best_fields", "most_fields", "cross_fields", "phrase", "phrase_prefix"] | None = None,
        tie_breaker: float | None = None,
        operator: Literal["and", "or"] | None = None,
        minimum_should_match: int | str | None = None,
        boost: float | None = None,
        **kwargs: Any,
    ) -> MultiMatchQuery:
        """Convenience builder that sets only provided keys (keeps JSON clean)."""
        payload: dict[str, Any] = {"query": query, "fields": fields}
        if type is not None:
            payload["type"] = type
        if tie_breaker is not None:
            payload["tie_breaker"] = tie_breaker
        if operator is not None:
            payload["operator"] = operator
        if minimum_should_match is not None:
            payload["minimum_should_match"] = minimum_should_match
        if boost is not None:
            payload["boost"] = boost
        payload.update(kwargs)
        return cls(multi_match=payload)


class TermQuery(BaseModel):
    """
    Container for a `term` query (exact match on a single field).

    JSON shapes:
      Compact: {"term": {"status": "active"}}
      Verbose: {"term": {"status": {"value": "active", "boost": 2.0}}}

    LLM usage:
      - Use for keyword/numeric/boolean/date fields (non-analyzed exact).
    """
    term: dict[str, JsonScalar | TermValue] = Field(
        ..., description="Map of one field name to an exact value or TermValue object."
    )
    model_config = ConfigDict(extra="forbid")


class TermsQuery(BaseModel):
    """
    Container for a `terms` query (IN-list exact match).

    JSON shape:
      {"terms": {"status": ["draft", "published"]}}
    """
    terms: dict[str, list[JsonScalar]] = Field(
        ..., description="Map of one field name to a list of allowed values."
    )
    model_config = ConfigDict(extra="forbid")


class RangeQuery(BaseModel):
    """
    Container for a `range` query.

    JSON shape:
      {"range": {"timestamp": {"gte": "2025-01-01", "lt": "2026-01-01"}}}
    """
    range: dict[str, RangeOps] = Field(
        ..., description="Map of one field name to RangeOps operators."
    )
    model_config = ConfigDict(extra="forbid")


class ExistsQuery(BaseModel):
    """
    Container for an `exists` query (documents where a field is present).

    JSON shape:
      {"exists": {"field": "author"}}
    """
    exists: dict[Literal["field"], str] = Field(
        ..., description='Object with key "field" naming the field that must exist.'
    )
    model_config = ConfigDict(extra="forbid")


class MatchAllQuery(BaseModel):
    """
    Container for a `match_all` query (matches every document).

    JSON shapes:
      {"match_all": {}}
      {"match_all": {"boost": 1.2}}
    """
    match_all: dict[str, Any] = Field(
        default_factory=dict, description="Optional object; may contain 'boost'."
    )
    model_config = ConfigDict(extra="forbid")


class IdsQuery(BaseModel):
    """
    Container for an `ids` query (match by document IDs).

    JSON shape:
      {"ids": {"values": ["1","2","3"]}}
    """
    ids: dict[Literal["values"], list[str]] = Field(
        ..., description='Object with key "values" listing document IDs.'
    )
    model_config = ConfigDict(extra="forbid")


# -----------------------------------------------------------------------------
# Declare the forward-ref union as a STRING TypeAlias (no runtime union math)
# -----------------------------------------------------------------------------

Query: TypeAlias = (
    "MatchQuery | MultiMatchQuery | TermQuery | TermsQuery | "
    "RangeQuery | ExistsQuery | MatchAllQuery | IdsQuery | BoolQuery"
)


# -----------------------------------------------------------------------------
# Recursive bool query (uses the forward-ref "Query")
# -----------------------------------------------------------------------------

class BoolBody(BaseModel):
    """
    Inner body for a `bool` query. Combines other queries using boolean logic.
    """
    must: list[Query] | None = Field(None, description="All must match; contributes to score.")
    filter: list[Query] | None = Field(None, description="All must match; does NOT affect score.")
    should: list[Query] | None = Field(None, description="Optional; affects score unless minimum_should_match is set.")
    must_not: list[Query] | None = Field(None, description="Documents must NOT match these.")
    minimum_should_match: int | str | None = Field(
        None, description='Minimum number/percent of `should` clauses that must match, e.g. 1 or "75%".'
    )
    boost: float | None = Field(None, description="Score multiplier for the bool query as a whole.")

    model_config = ConfigDict(extra="forbid")


class BoolQuery(BaseModel):
    """
    Container for a `bool` query.
    """
    bool: BoolBody = Field(..., description="Boolean composition of other queries.")
    model_config = ConfigDict(extra="forbid")


# Resolve forward refs now that all classes are defined and the alias exists.
BoolBody.model_rebuild()
BoolQuery.model_rebuild()


# -----------------------------------------------------------------------------
# Search request wrapper (base)
# -----------------------------------------------------------------------------

class SearchRequest(BaseModel):
    """
    Minimal wrapper for the Elasticsearch `_search` request body.
    """
    query: Query = Field(..., description="A single Query DSL container (match, bool, range, etc.).")
    size: int | None = Field(None, description="Maximum number of hits to return.")
    from_: int | None = Field(default=None, alias="from", description="Offset for pagination (use with `size`).")
    model_config = ConfigDict(populate_by_name=True, extra="forbid")


# SearchRequest also references Query; resolve again to be safe.
SearchRequest.model_rebuild()


# -----------------------------------------------------------------------------
# Aggregations (Elasticsearch-compatible)
# -----------------------------------------------------------------------------

MAX_AGG_NESTING = 3
MAX_BUCKETS = 1000


class TermsAgg(BaseModel):
    """
    Bucket: terms aggregation.

    JSON shape:
      {"terms": {"field": "user", "size": 100, "order": {"_count": "desc"}}}
    """
    field: str = Field(..., description="Keyword/numeric/boolean field to bucket by.")
    size: int | None = Field(None, description=f"Max buckets to return (<= {MAX_BUCKETS}).")
    shard_size: int | None = Field(None, description="Candidate buckets per shard.")
    min_doc_count: int | None = Field(None, description="Minimum doc count per bucket.")
    order: dict[str, Literal["asc", "desc"]] | None = Field(None, description="Sort buckets, e.g. {'_count':'desc'}")
    include: str | list[str] | None = Field(None, description="Regex or values to include.")
    exclude: str | list[str] | None = Field(None, description="Regex or values to exclude.")
    missing: JsonScalar | None = Field(None, description="Bucket docs with missing field value.")
    script: dict[str, Any] | None = Field(None, description="Painless script object.")
    show_term_doc_count_error: bool | None = None
    execution_hint: Literal["map", "global_ordinals", "bytes_hash"] | None = None

    model_config = ConfigDict(extra="forbid")

    @model_validator(mode="after")
    def _enforce_size_cap(self) -> "TermsAgg":
        if self.size is not None and self.size > MAX_BUCKETS:
            raise ValueError(f"terms.size must be <= {MAX_BUCKETS}")
        return self


class DateHistogramAgg(BaseModel):
    """
    Bucket: date_histogram.

    JSON shape:
      {"date_histogram": {"field": "@timestamp", "calendar_interval": "1d"}}

    Notes:
      - Provide exactly one of calendar_interval or fixed_interval.
    """
    field: str
    calendar_interval: str | None = None  # e.g. '1d', '1w', '1M', '1y'
    fixed_interval: str | None = None     # e.g. '1m', '10m', '1h'
    offset: str | None = None
    min_doc_count: int | None = None
    time_zone: str | None = None
    extended_bounds: dict[str, JsonScalar] | None = Field(
        None, description="{'min': <date/datetime>, 'max': <date/datetime>}"
    )
    hard_bounds: dict[str, JsonScalar] | None = None
    order: dict[str, Literal["asc", "desc"]] | None = None  # recent ES supports ordering

    model_config = ConfigDict(extra="forbid")

    @model_validator(mode="after")
    def _interval_choice(self) -> "DateHistogramAgg":
        if bool(self.calendar_interval) == bool(self.fixed_interval):
            raise ValueError("Provide exactly one of calendar_interval or fixed_interval")
        return self


class HistogramAgg(BaseModel):
    """
    Bucket: numeric histogram.

    JSON shape:
      {"histogram": {"field": "latency_ms", "interval": 50}}
    """
    field: str
    interval: float
    min_doc_count: int | None = None
    offset: float | None = None
    extended_bounds: dict[str, float] | None = None
    hard_bounds: dict[str, float] | None = None

    model_config = ConfigDict(extra="forbid")


class RangeSpec(BaseModel):
    from_: JsonScalar | None = Field(None, alias="from")
    to: JsonScalar | None = None
    key: str | None = None

    model_config = ConfigDict(populate_by_name=True, extra="forbid")


class RangeAgg(BaseModel):
    """
    Bucket: range aggregation.

    JSON shape:
      {"range": {"field": "bytes", "ranges": [{"to": 1000}, {"from": 1000, "to": 10000}]}}
    """
    field: str
    ranges: list[RangeSpec]

    model_config = ConfigDict(extra="forbid")

    @model_validator(mode="after")
    def _bucket_cap(self) -> "RangeAgg":
        if len(self.ranges) > MAX_BUCKETS:
            raise ValueError(f"range.ranges length must be <= {MAX_BUCKETS}")
        return self


class FiltersAgg(BaseModel):
    """
    Bucket: filters aggregation (named filters only so we can count them).

    JSON shape:
      {"filters": {"filters": {"ok": {"term": {"status": "ok"}}, "ko": {"term": {"status": "ko"}}}}}
    """
    filters: dict[str, Query] = Field(..., description="Named filters map.")

    model_config = ConfigDict(extra="forbid")

    @model_validator(mode="after")
    def _bucket_cap(self) -> "FiltersAgg":
        if len(self.filters) > MAX_BUCKETS:
            raise ValueError(f"filters must define <= {MAX_BUCKETS} named filters")
        return self


# --- Metric aggregations (do not create buckets; no bucket cap needed) ---

class AvgAgg(BaseModel):
    avg: dict[Literal["field"], str]
    model_config = ConfigDict(extra="forbid")


class SumAgg(BaseModel):
    sum: dict[Literal["field"], str]
    model_config = ConfigDict(extra="forbid")


class MinAgg(BaseModel):
    min: dict[Literal["field"], str]
    model_config = ConfigDict(extra="forbid")


class MaxAgg(BaseModel):
    max: dict[Literal["field"], str]
    model_config = ConfigDict(extra="forbid")


class StatsAgg(BaseModel):
    stats: dict[Literal["field"], str]
    model_config = ConfigDict(extra="forbid")


class CardinalityAgg(BaseModel):
    cardinality: dict[str, Any]  # {'field': '...', 'precision_threshold': 100}
    model_config = ConfigDict(extra="forbid")


# -----------------------------------------------------------------------------
# Aggregation container (one-of per node) + recursive sub-aggregations
# -----------------------------------------------------------------------------

class Aggregation(BaseModel):
    """
    A single aggregation node (exactly one of the following must be present),
    with optional sub-aggregations via `aggs`.
    """
    # Bucket aggs
    terms: TermsAgg | None = None
    date_histogram: DateHistogramAgg | None = None
    histogram: HistogramAgg | None = None
    range: RangeAgg | None = None
    filters: FiltersAgg | None = None

    # Metric aggs
    avg: dict[Literal["field"], str] | None = None
    sum: dict[Literal["field"], str] | None = None
    min: dict[Literal["field"], str] | None = None
    max: dict[Literal["field"], str] | None = None
    stats: dict[Literal["field"], str] | None = None
    cardinality: dict[str, Any] | None = None

    # Sub-aggregations
    aggs: dict[str, "Aggregation"] | None = Field(
        None, description="Sub-aggregations (counts towards nesting limit)."
    )

    model_config = ConfigDict(extra="forbid")

    @model_validator(mode="after")
    def _one_of(self) -> "Aggregation":
        present = [
            name
            for name, val in self.__dict__.items()
            if name not in {"aggs"} and val is not None
        ]
        if len(present) != 1:
            raise ValueError(
                "Each aggregation node must define exactly one aggregation type "
                f"(found {present or 'none'})."
            )
        return self


Aggregation.model_rebuild()  # forward ref for self-referencing 'aggs'


class AggregationsRoot(BaseModel):
    """
    Top-level 'aggs' object with max nesting enforcement.
    """
    aggs: dict[str, Aggregation] = Field(default_factory=dict)

    model_config = ConfigDict(extra="forbid")

    @staticmethod
    def _compute_depth(node: Aggregation, current: int = 1) -> int:
        """Depth of this node including itself; children add +1."""
        if not node.aggs:
            return current
        return max(AggregationsRoot._compute_depth(child, current + 1) for child in node.aggs.values())

    @model_validator(mode="after")
    def _enforce_depth_limit(self) -> "AggregationsRoot":
        for name, agg in self.aggs.items():
            depth = self._compute_depth(agg, current=1)
            if depth > MAX_AGG_NESTING:
                raise ValueError(
                    f"Aggregation '{name}' exceeds max nesting depth "
                    f"{MAX_AGG_NESTING} (found depth {depth})."
                )
        return self


# -----------------------------------------------------------------------------
# Search request with aggregations
# -----------------------------------------------------------------------------

class SearchRequestWithAggs(SearchRequest):
    """
    `_search` request wrapper with optional aggregations.
    """
    aggs: dict[str, Aggregation] | None = Field(
        default=None,
        description="Top-level aggregations ('aggs' object)."
    )

    model_config = ConfigDict(populate_by_name=True, extra="forbid")

    @model_validator(mode="after")
    def _validate_tree(self) -> "SearchRequestWithAggs":
        if self.aggs:
            # Reuse AggregationsRoot validator (depth + bucket caps enforced in nodes)
            AggregationsRoot(aggs=self.aggs)
        return self


# -----------------------------------------------------------------------------
# Optional: quick self-test when run directly
# -----------------------------------------------------------------------------

if __name__ == "__main__":
    # Sanity: valid bool+aggs
    q = BoolQuery(
        bool=BoolBody(
            must=[MatchQuery(match={"title": MatchFieldOptions(query="wind power", operator="and")})],
            filter=[RangeQuery(range={"@timestamp": RangeOps(gte="now-7d/d", lt="now/d")})],
            should=[TermQuery(term={"lang": "en"}), TermsQuery(terms={"category": ["energy", "climate"]})],
            minimum_should_match=1,
        )
    )
    req = SearchRequestWithAggs(
        query=q,
        size=10,
        aggs={
            "by_lang": Aggregation(
                terms=TermsAgg(field="lang", size=10),
                aggs={
                    "per_day": Aggregation(
                        date_histogram=DateHistogramAgg(field="@timestamp", calendar_interval="1d"),
                        aggs={"bytes_stats": Aggregation(stats={"field": "bytes"})},
                    )
                },
            )
        },
        **{"from": 0},
    )
    print(req.model_dump(by_alias=True, exclude_none=True))